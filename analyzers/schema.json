{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "properties": {
    "text": {
      "type": "string",
      "description": "The input text"
    },
    "analyzer": {
      "type": "string",
      "$ref": "#/definitions/esanalyze"
    },
    "char_filter": {
      "type": "array",
      "description": "Convert original input text into a stream of characters then preprocess it before passing it as an input to tokenizer",
      "items": {
        "anyOf": [
          { "$ref": "#/definitions/escharfilter" },
          { "$ref": "#/definitions/escharfiltermap" },
          { "$ref": "#/definitions/escharfilterpattern" }
        ]
      }
    },
    "tokenizer": {
      "description": "Split the Output of Character Filters into Unique Terms",
      "anyOf": [
        { "$ref": "#/definitions/estokenizer" },
        { "$ref": "#/definitions/esngram" },
        { "$ref": "#/definitions/estokenizerstructpattern" },
        { "$ref": "#/definitions/estokenizerchargroup" },
        { "$ref": "#/definitions/estokenizerpathhierarchy" }
      ]
    },
    "filter": {
      "type": "array",
      "items": {
        "anyOf": [
          { "$ref": "#/definitions/estokenfilter" },
          { "$ref": "#/definitions/estokenfilter_asciifolding" },
          { "$ref": "#/definitions/esngram" },
          { "$ref": "#/definitions/estokenfilter_fingerprint" },
          { "$ref": "#/definitions/estokenfilter_keep" },
          { "$ref": "#/definitions/estokenfilter_keeptypes" },
          { "$ref": "#/definitions/estokenfilter_stemmer" }
        ]
      }
    }
  },
  "definitions": {
    "esanalyze": {
      "type": "string",
      "default": "standard",
      "enum": [
        "standard",
        "simple",
        "whitespace",
        "stop",
        "keyword",
        "pattern",
        "arabic",
        "armenian",
        "basque",
        "bengali",
        "brazilian",
        "bulgarian",
        "catalan",
        "cjk",
        "czech",
        "danish",
        "dutch",
        "english",
        "estonian",
        "finnish",
        "french",
        "galician",
        "german",
        "greek",
        "hindi",
        "hungarian",
        "indonesian",
        "irish",
        "italian",
        "latvian",
        "lithuanian",
        "norwegian",
        "persian",
        "portuguese",
        "romanian",
        "russian",
        "sorani",
        "spanish",
        "swedish",
        "turkish",
        "thai",
        "kuromoji",
        "smartcn",
        "openkoreantext-analyzer"
      ]
    },

    "escharfilter": {
      "type": "string",

      "enum": ["html_strip", "pattern_replace"]
    },
    "escharfiltermap": {
      "type": "object",
      "properties": {
        "type": {
          "type": "string",
          "enum": ["mapping"]
        },
        "mappings": {
          "type": "array",
          "items": {
            "type": "string"
          }
        }
      }
    },
    "escharfilterpattern": {
      "type": "object",
      "properties": {
        "type": {
          "type": "string",
          "enum": ["pattern_replace"]
        },
        "pattern": {
          "description": "Regular expression for finding token",
          "type": "string"
        },
        "replacement": {
          "description": "Regular expression for replacing token",
          "type": "string"
        }
      }
    },
    "estokenizer": {
      "type": "string",
      "default": "standard",
      "enum": [
        "standard",
        "letter",
        "lowercase",
        "whitespace",
        "uax_url_email",
        "classic",
        "thai",
        "keyword",
        "pattern"
      ]
    },
    "esngram": {
      "type": "object",
      "properties": {
        "type": {
          "type": "string",
          "enum": ["ngram", "edge_ngram"]
        },
        "min_gram": {
          "description": "This slides along the input character stream to provide items in the specified length of the specified characters. It uses min_gram (this defaults to 1) and max_gram(this defaults to 2) to specify the length",
          "type": "integer",
          "default": 1
        },
        "max_gram": {
          "description": "This slides along the input character stream to provide items in the specified length of the specified characters. It uses min_gram (this defaults to 1) and max_gram(this defaults to 2) to specify the length",
          "type": "integer",
          "default": 2
        },
        "token_chars": {
          "description": "This slides along the input character stream to provide items in the specified length of the specified characters. token_chars is used to specify the letters, digits, whitespace, punctuation, and symbol.",
          "type": "array",
          "items": {
            "$ref": "#/definitions/estokentype"
          }
        }
      }
    },
    "estokenizerstructpattern": {
      "type": "object",
      "properties": {
        "type": {
          "type": "string",
          "enum": ["pattern", "simple_pattern", "simple_pattern_split"]
        },
        "pattern": {
          "description": "Specify regular expression",
          "type": "string",
          "default": "\\W+"
        },
        "flag": {
          "description": "Specify the flag of the Java regular expression",
          "type": "string"
        },
        "group": {
          "description": "Specify the group matched",
          "type": "string"
        }
      }
    },
    "estokentype": {
      "type": "string",
      "enum": ["letter", "digit", "whitespace", "punctuation", "symbol"]
    },
    "estokenizerchargroup": {
      "type": "object",
      "properties": {
        "type": {
          "type": "string",
          "enum": ["char_group"]
        },
        "tokenize_on_chars": {
          "description": "You can define a set of separators to split the input character stream into terms. Use tokenize_on_chars to specify a list of separators.",
          "type": "array",
          "items": {
            "$ref": "#/definitions/estokentype"
          }
        },
        "flag": {
          "description": "Specify the flag of the Java regular expression",
          "type": "string"
        },
        "group": {
          "description": "Specify the group matched",
          "type": "string"
        }
      }
    },
    "estokenizerpathhierarchy": {
      "type": "object",
      "properties": {
        "type": {
          "type": "string",
          "enum": ["path_hierarchy"]
        },
        "replacement": {
          "description": "This uses the path separator to split the input character stream into terms - replacement: the character to replace the delimiter",
          "type": "string"
        },
        "delimiter": {
          "description": "This uses the path separator to split the input character stream into terms - delimiter: the separator",
          "type": "string"
        },
        "buffer_size": {
          "description": "This uses the path separator to split the input character stream into terms - buffer_size: the maximum length in one batch",
          "type": "integer"
        },
        "reverse": {
          "description": "This uses the path separator to split the input character stream into terms - reverse: this reverses the generated terms",
          "type": "boolean"
        },
        "skip": {
          "description": "This uses the path separator to split the input character stream into terms - skip: the number of generated terms to skip",
          "type": "integer"
        }
      }
    },
    "estokenfilter": {
      "type": "string",
      "default": "lowercase",
      "enum": ["lowercase", "uppercase", "fingerprint"]
    },
    "estokenfilter_asciifolding": {
      "type": "object",
      "properties": {
        "type": {
          "type": "string",
          "enum": ["asciifolding"]
        },
        "preserve_original": {
          "type": "boolean",
          "default": false,
          "description": "This transforms the terms when letters, numbers, and unicode symbols are not in the first 127 ASCII characters to ASCII - retain the original terms"
        }
      }
    },
    "estokenfilter_fingerprint": {
      "type": "object",
      "properties": {
        "type": {
          "type": "string",
          "enum": ["fingerprint"]
        },
        "separator": {
          "type": "string",
          "default": " ",
          "description": "Sort, deduplicate, and concatenate the terms from the tokenizer into one term. - The separator parameter (this defaults to the space character) can be set to another character. "
        },
        "max_output_size": {
          "type": "integer",
          "default": 255,
          "description": "Sort, deduplicate, and concatenate the terms from the tokenizer into one term. - The max_output_size parameter (this defaults to 255) will restrict the emitting of the final concatenated term. "
        }
      }
    },
    "estokenfilter_keep": {
      "type": "object",
      "properties": {
        "type": {
          "type": "string",
          "enum": ["keep"]
        },
        "keep_words": {
          "description": "Only those terms defined in the list of specified words are kept. Three options are provided: the keep_words parameter allows you to specify a list of words in the filter ",
          "type": "array",
          "items": {
            "type": "string"
          }
        },
        "keep_words_path": {
          "type": "array",
          "description": "the keep_words_path parameter allows you to specify a list of words in the file path; ",
          "items": {
            "type": "string"
          }
        },
        "keep_words_case": {
          "type": "boolean",
          "default": false,
          "description": "the keep_words_case parameter (this defaults to false) converts the terms to lowercase. "
        }
      }
    },
    "estokenfilter_keeptypes": {
      "type": "object",
      "properties": {
        "type": {
          "type": "string",
          "enum": ["keep_types"]
        },
        "types": {
          "description": "Only those terms defined in the list of specified token types are kept. ",
          "type": "array",
          "items": {
            "type": "string",
            "enum": ["<NUM>", "<ALPHANUM>", "<HANGUL>", "word", "<SYNONYM>"]
          }
        },
        "mode": {
          "type": "string",
          "description": "allows you to include or exclude the specified terms",
          "default": "include",
          "enum": ["include", "exclude"]
        }
      }
    },
    "estokenfilter_stemmer": {
      "type": "object",
      "properties": {
        "type": {
          "type": "string",
          "enum": ["stemmer"]
        },
        "name": {
          "description": "Allows to specify stemmer in different languages and apply it to the terms ",
          "$ref": "#/definitions/eslanguage"
        },
        "mode": {
          "type": "string",
          "description": "allows you to include or exclude the specified terms",
          "default": "include",
          "enum": ["include", "exclude"]
        }
      }
    },
    "eslanguage": {
      "type": "string",
      "enum": [
        "arabic",
        "armenian",
        "basque",
        "bengali",
        "brazilian",
        "bulgarian",
        "catalan",
        "czech",
        "danish",
        "dutch",
        "dutch_kp",
        "english",
        "light_english",
        "lovins",
        "minimal_english",
        "porter2",
        "possessive_english",
        "estonian",
        "finnish",
        "light_finnish",
        "french",
        "light_french",
        "minimal_french",
        "galician",
        "minimal_galician",
        "german",
        "german2",
        "minimal_german",
        "light_german",
        "greek",
        "hindi",
        "hungarian",
        "light_hungarian",
        "indonesian",
        "irish",
        "italian",
        "light_italian",
        "latvian",
        "lithuanian",
        "norwegian",
        "light_norwegian",
        "minimal_norwegian",
        "light_nynorsk",
        "minimal_nynorsk",
        "portuguese",
        "light_portuguese",
        "minimal_portuguese",
        "portuguese_rslp",
        "romanian",
        "russian",
        "light_russian",
        "sorani",
        "spanish",
        "light_spanish",
        "swedish",
        "light_swedish",
        "turkish"
      ]
    }
  }
}
